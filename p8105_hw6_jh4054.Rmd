---
title: "p8105_hw6_jh4054"
author: "Joy Hsu"
date: "11/24/2018"
output: 
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)

knitr::opts_chunk$set(
  collapse = TRUE)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

### Problem 1

The homicide dataset was collated by Washington Post on 52179 homicide cases in 50 major US cities, from 2007 to 2015. Altogether, 28 states are represented. The raw dimensions are 52179 rows by 12 columns variables. Each case is reported as a distinct row, with demographic information on the victim's name, age, sex, and race. Attributes regarding the homicide incident include geospatial coordinates, reported date, city, and disposition of the police case. 
```{r}
#examine raw dataset
raw_dataset = read_csv("./data/homicide-data.csv")
dim(raw_dataset)

#represented states and cities
raw_dataset %>% 
  distinct(state) %>% 
  nrow()

raw_dataset %>% 
  distinct(city) %>% 
  nrow()
```

Load and tidy dataset 

* created variable city_state
* omit "Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"
* modify victim age to numeric variable
* modify victim_race to two factor variable: white (reference) and non_white
* create binary variable indicating whether the homicide is solved: 0 = unsolved, 1 = solved. 

```{r}
#load and tidy dataset
homicide_data = read_csv("./data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    victim_age = as.numeric(as.factor(victim_age)),
    victim_race = ifelse(victim_race == "White", "white", "non_white"),
    victim_race = fct_relevel(victim_race, c("white", "non_white")),
    victim_sex = as.factor(victim_sex),
    hom_unsolved = as.factor(ifelse(disposition != "Closed by arrest", 0, 1))) %>% 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO")))
```

```{r}
# df with only baltimore victims
baltimore_df = homicide_data %>% 
   filter(city_state == "Baltimore, MD") 

# glm for baltimore unsolved homicides
baltimore_df %>%
  glm(hom_unsolved ~ victim_age + victim_race + victim_sex, family = binomial(), data = .) %>% 
  broom::tidy(conf.int = TRUE, exponentiate = TRUE) %>% 
  select(term, estimate, conf.low, conf.high, p.value) %>% 
  rename(
    "OR" = estimate, 
    "95% CI lower" = conf.low, 
    "95% CI upper" = conf.high) %>% 
  knitr::kable(digits = 3)
```

```{r, fig.width = 10, fig.height = 8}
# OR for victim_racenon_white
logfit_cities = homicide_data %>% 
  select(city_state, hom_unsolved, victim_age, victim_race, victim_sex) %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(
    logfit = map(data, ~glm(hom_unsolved ~ victim_age + victim_race + victim_sex, family = binomial(), data = .)),
    logfit = map(logfit, ~broom::tidy(., conf.int = TRUE, exponentiate = TRUE))) %>% 
  select(city_state, logfit) %>% 
  unnest() %>% 
  filter(term == "victim_racenon_white") %>% 
  select(city_state, estimate, conf.low, conf.high, p.value) %>% 
  rename(
    "OR" = estimate, 
    "CI_lower" = conf.low, 
    "CI_upper" = conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, OR))

# OR and 95% CI for each city
logfit_cities %>% 
  arrange(OR) %>% 
  knitr::kable(digits = 3)

# plot
OR_plot = logfit_cities %>% 
  ggplot(aes(x = city_state, y = OR, color = city_state)) +
  geom_point() +
  geom_errorbar(mapping = aes(x = city_state, ymin = CI_lower, ymax = CI_upper)) + 
  labs(
    title = "Odds Ratio of Solving Homicide Case, Race non-white vs. white",
    x = "City",
    y = "Odds Ratio",
    caption = "*Case Data collated by Washington Post") + 
  viridis::scale_color_viridis(
    name = "City, State", 
    discrete = TRUE) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        legend.position = "bottom")
OR_plot
```

### Problem 2

Exclude two variables that have values of "0" for all observations. 

* pnumlbw: previous number of low birth weight babies
* pnumgsa: number of prior small for gestational age babies

```{r}
bw = read_csv("./data/birthweight.csv")

bw = bw %>% 
  mutate(
    babysex = as.factor(recode(babysex, "1" = "male", "2" = "female")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8), labels = c("white", "black", "asian", "puerto_rican", "other")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4), labels = c("white", "black", "asian", "puerto_rican")),
    malform = factor(malform, levels = c(0, 1), labels = c("absent", "present"))) %>% 
  select(-pnumlbw, -pnumsga)

str(bw)

fit1 = lm(bwt ~ ., data = bw)
summary(fit1)
fit1 %>% broom::tidy() %>% arrange(p.value)
```

### Histogram for bw

Check normality distribution of the response variable birthweight

```{r}
bw %>% 
  ggplot(aes(x = bwt)) +
  geom_histogram()
```

### Correlation Matrix for bw

* identify collinear variables, exclude from model selection process

```{r, eval=FALSE}
GGally::ggpairs(bw)
pairs(bw)
cor(bw)
```

### Stepwise Selection

```{r}
library(leaps)

# Specify full and null models for forward & backward selection based on AIC Criteria
null = lm(bwt ~ 1, data = bw)
null

full = lm(bwt ~ ., data = bw)
full
```

**Stepwise Forward Selection**

Best Model that minimizes AIC. According to this procedure, the best model is the one that includes the variables *bhead + blength + mrace + delwt + gaweeks + smoken + ppbmi + babysex + parity + ppwt + fincome*

Step:  AIC=48706.22
bwt ~ bhead + blength + mrace + delwt + gaweeks + smoken + ppbmi + 
    babysex + parity + ppwt + fincome

```{r}
# command for forward selection
# This tells R to start with the null model and search through models lying in the range between the null and full model using the forward selection algorithm. It gives rise to the following output:
step(null, scope=list(lower=null, upper=full), direction="forward")
```

**Stepwise Backward Elimination**

Algorithm give rise to results are equivalent to the forward selection procedure. 

Step:  AIC=48705.38
bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + 
    mheight + mrace + parity + ppwt + smoken

```{r}
# perform stepwise backward selection
step(full, data = bw, direction="backward")
```

**stepwise regression, both directions**

Step:  AIC=48706.22
bwt ~ bhead + blength + mrace + delwt + gaweeks + smoken + ppbmi + 
    babysex + parity + ppwt + fincome

```{r}
step(null, scope = list(upper=full), data = bw, direction="both")
```

### **regsubsets()** - regression subset selection

The all possible regressions approach considers all possible subsets of the pool
of explanatory variables and finds the model that best fits the data according to
some criteria (e.g. Adjusted R2, AIC and BIC). These criteria assign scores to
each model and allow us to choose the model with the best score.

* The model containing bhead, blength, delwt, frace:puerto_rican, gaweeks, mraceblack, ppbmi, and smoken minimizes the **adjusted R-square criteria**. The top 5 models in the R~2~(adj) plot have roughly the same R~2~(adj).
* The same model containing variables bhead, blength, delwt, frace:puerto_rican, gaweeks, mraceblack, ppbmi, and smoken also minimizes the **BIC criteria**.
* There is not significant improvement in R~2~(adj) by keeping frace:puerto_rican. * From this exploration, we will retain the following as a candidate model

**bwt ~ bhead + blength + delwt + gaweeks + mrace + ppbmi + smoken**

```{r}
leaps = leaps::regsubsets(bwt ~ ., data = bw)

fit2 = lm(bwt ~ bhead + blength + delwt + gaweeks + mrace + ppbmi + smoken, data = bw)
summary(fit2)

# View the ranked models according to the adjusted r-squared criteria and BIC, respectively
# black indicates that a variable is included in the model, white not included. 
# Model containing babysex, bhead, blength, delwt, gaweeks, smoken, pnumsga
plot(leaps, scale = "adjr2")
plot(leaps, scale = "bic")
```

### Check Model Diagnostics for 2 Candidate Models

**Candidate Models:**

1. Stepwise Backward Elimination (same model as forward selection and stepwise regression)
bwt ~ bhead + blength + mrace + delwt + gaweeks + smoken + ppbmi + babysex + parity + ppwt + fincome

2. Based on adjusted R-square criteria. Removed frace since adds little benefit to adjR2. Keep all of mrace, since cannot only include one factor in variable
bwt ~ babysex + bhead + blength + delwt + frace:puerto_rican + gaweeks + mraceblack + smoken + pnumsga 

```{r}
# Candidate 1: Stepwise Backward Elimination Model, 11 predictors
fit_step = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + 
    mheight + mrace + parity + ppwt + smoken, data = bw)
summary(fit_step)

# add predictions and residuals to stepwise model
bw_fit_step = modelr::add_predictions(bw, fit_step, var = "pred_step")
bw_fit_step = modelr::add_residuals(bw_fit_step, fit_step, var = "resid_step")

# Candidate 2: adjusted R-square Model, 7 predictors
fit_rs = lm(bwt ~ bhead + blength + delwt + gaweeks + mrace + ppbmi + smoken, data = bw)
summary(fit_rs)

# add predictions and residuals to adjusted r-square model
bw_fit_rs = modelr::add_predictions(bw, fit_rs, var = "pred_rs")
bw_fit_rs = modelr::add_residuals(bw_fit_rs, fit_rs, var = "resid_rs")
```

**Check Conditions:**

1. linear relationship between (numerical) x and y
    * each numerical explanatory variable should be linearly related to the response variable, birthweight
    * check using residuals plot (e vs. x)
    * look for random scatter around 0
2. nearly normal residuals with mean 0
    * on a residuals plot, we look for random scatter of residuals around 0, this translates to a nearly normal distribution of residuals centered at 0
    * check using histogram of residuals or normal probability plot
3. constant variability of residuals 
    * check using residuals plots of residuals vs. predicted value (e vs. y_hat)
        * residuals should be equally variable for low and high values of the predicted response variable
        * residuals randomly scattered in a band with a constant width around 0 (no fan shaped)
4. independent residuals 

#### 1. linear relationship between explanatory variables (x) and response variable (y)

Don't need to do this for assignment

#### 2. Normality of Residuals (error distribution)

Acceptible normality per Normal Q-Q Plot and Histogram of Residuals

###### **Stepwise Model QQ-Plot & Histogram**
```{r}
# evaluate normality of residuals with qq plot
qqnorm(fit_step$residuals)
qqline(fit_step$residuals)

# evaluate normality of residuals with histogram
hist(fit_step$residuals)

bw_fit_step %>% 
  ggplot(aes(x = resid_step)) +
  geom_histogram()
```

###### **R-Square Model QQ-Plot & Histogram**
```{r}
# evaluate normality of residuals with qq plot
qqnorm(fit_rs$residuals)
qqline(fit_rs$residuals)

# evaluate normality of residuals with histogram
hist(fit_rs$residuals)

bw_fit_rs %>% 
  ggplot(aes(x = resid_step)) +
  geom_histogram()
```

#### 3. **Homoscedasticity: Constant variance of residuals (errors)**

```{r}
bw_fit_rs %>% 
  ggplot(aes(x = pred_rs, y = resid_rs)) +
  geom_point(alpha = 0.3)

bw_fit_step %>% 
  ggplot(aes(x = pred_step, y = resid_step)) +
  geom_point(alpha = 0.3)

# stepwise model: residuals plot of residuals vs. predicted value
plot(fit_step$residuals ~ fit_step$fitted.values)

# adjusted r-square model: residuals plot of residuals vs. predicted value
plot(fit_rs$residuals ~ fit_rs$fitted.values)
```

#### 4. Independence of residuals (errors) - NA for assignment

```{r}
plot(fit_step$residuals)
plot(fit_rs$residuals)
```

### Select Model

Both the backwards elimination model and adjusted r-squared model satisfied assumptions for 1) normality of residuals and 2) constant variance of residuals.

We prefer the parsimonious model, the simplest best model. Thus, we will select the R~2~(adj) model with 7 predictors over the backwards elimination model with 11 predictors.

Lastly, we will check collinearity of variables in the 7 predictor model using a 1) correlation matrix and 2) VIF (variance inflation factor) Scores, which measure how much of the variance of a regression coefficient is inflated due to multicollinearity within the model. 

```{r}
fit_rs %>% broom::tidy() %>% knitr::kable()
fit_rs %>% broom::glance() %>% knitr::kable()
summary(fit_rs)

# check for collinearity between numerical predictor variables
bw_fit_rs %>% 
  select(bwt, bhead, blength, delwt, gaweeks, ppbmi, smoken) %>% 
  GGally::ggpairs()

# Evaluate VIF Scores
car::vif(fit_rs)
```

### Cross-Validation Compare Models

```{r}
# two predictor model
fit_2 = lm(bwt ~ blength, gaweeks, data = bw)
summary(fit_2)

# 3 way interaction model
fit_3 = lm(bwt ~ bhead * blength * babysex, data = bw)
summary(fit_3)
```

```{r}
cv_bw = modelr::crossv_mc(data = bw, n = 100, test = 0.2, id = "id")

cv_bw = cv_bw %>% 
  mutate
```


