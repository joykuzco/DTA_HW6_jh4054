---
title: "p8105_hw6_jh4054"
author: "Joy Hsu"
date: "11/24/2018"
output: 
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  collapse = TRUE)

set.seed(1)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

### Problem 1

The homicide dataset was collated by Washington Post on homicide cases in 50 major US cities, from 2007 to 2015. We will investigate the odds ratio of solving a homicide for white victims versus non-white victims. 

To begin, we tidy the dataset as follows:

* create city_state variable
* omit observations from "Dallas, TX", "Phoenix, AZ", "Kansas City, MO" since these locations do not report victim race. 
* Omit single "Tulsa, AL" observation due to erroneous entry. 
* modify victim age to numeric variable
* modify victim_race to two factor variable: white (reference) and non_white
* modify victim_sex to factor variable: Male, Female, Unknown
* create binary variable indicating whether homicide case is resolved: 0 = unsolved case, 1 = resolved case ("disposition = Closed by arrest") 

```{r}
#load and tidy dataset
homicide_data = read_csv("./data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    victim_age = as.numeric(as.factor(victim_age)),
    victim_race = ifelse(victim_race == "White", "white", "non_white"),
    victim_race = fct_relevel(victim_race, c("white", "non_white")),
    victim_sex = as.factor(victim_sex),
    hom_unsolved = as.factor(ifelse(disposition != "Closed by arrest", 0, 1))) %>% 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO")))
```

### Problem 1.1 Baltimore, MD

For the city of Baltimore, MD, we fit a logistic regression with resolved versus unsolved homicide as the outcome and victim age, sex, and race (white vs. non-white) as predictors. The odds of resolving a homicide case for a non-white victim is 0.452 (Odds Ratio 95% CI: 0.321, 0.636) times the odds of solving a homicide case for a white homicide victim, in Baltimore, keeping all other variables constant.

```{r}
# filter dataframe for baltimore victims only
baltimore_df = homicide_data %>% 
   filter(city_state == "Baltimore, MD") 

# save glm for baltimore unsolved homicides as object
glm_baltimore = baltimore_df %>%
  glm(hom_unsolved ~ victim_age + victim_race + victim_sex, family = binomial(), data = .)

# table for adjusted odds ratio, 95% Confidence Interval, p-value 
glm_baltimore %>% broom::tidy(conf.int = TRUE, exponentiate = TRUE) %>% 
  select(term, estimate, conf.low, conf.high, p.value) %>% 
  rename(
    "Odds_Ratio" = estimate, 
    "95% CI lower" = conf.low, 
    "95% CI upper" = conf.high) %>% 
  filter(term == "victim_racenon_white") %>% 
  knitr::kable(digits = 3)
```

### Problem 1.2 All Cities

The Odds Ratio and 95% Confidence Interval for solving a homicide case for a non-white victim versus a white victim is reported for 47 major US cities.

```{r, fig.width = 10, fig.height = 8}
# Odds Ratio for victim_racenon_white in 47 cities
logfit_cities = homicide_data %>% 
  select(city_state, hom_unsolved, victim_age, victim_race, victim_sex) %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(
    logfit = map(data, ~glm(hom_unsolved ~ victim_age + victim_race + victim_sex, family = binomial(), data = .)),
    logfit = map(logfit, ~broom::tidy(., conf.int = TRUE, exponentiate = TRUE))) %>% 
  select(city_state, logfit) %>% 
  unnest() %>% 
  filter(term == "victim_racenon_white") %>% 
  select(city_state, estimate, conf.low, conf.high, p.value) %>% 
  rename(
    "OR" = estimate, 
    "CI_lower" = conf.low, 
    "CI_upper" = conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, OR))

# OR and 95% CI for each city
logfit_cities %>% 
  arrange(OR) %>% 
  knitr::kable(digits = 3)
```

44 out of the 47 major cities had an odds ratio under 1, comparing the odds of resolving homicide case for non-white victim versus white victim. In other words, the likelihood of resolving a homicide event for a non-white victim is consistently less than the likelihood of resolving a homicide event of a white victim, across most major US cities.   

In only three cities (Durham, NC; Birmingham, AL; Tampa, FL), the odds ratio was greater than 1. However, the CI lower limit for the odds ratio is less than 1 in these locations.

```{r, fig.width = 10, fig.height = 6}
# Odds Ratio plot for all cities
logfit_cities %>% 
  ggplot(aes(x = city_state, y = OR, color = city_state)) +
  geom_point() +
  geom_errorbar(mapping = aes(x = city_state, ymin = CI_lower, ymax = CI_upper)) + 
  labs(
    title = "Odds Ratio of Solving Homicide Case, Race non-white vs. white",
    x = "City",
    y = "Odds Ratio",
    caption = "*Case Data collated by Washington Post") + 
  viridis::scale_color_viridis(
    name = "City, State", 
    discrete = TRUE) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        legend.position = "none")
```

### Problem 2.1 - Propose Model

In this section, we will investigate the birthweight dataset and propose a regression model for birthweight. 

First, we will explore the distribution of the main response. Next, we will select 2 model candidates from 1) stepwise backwards elimination and 2) optimization of the adjusted R~2~ criteria. Candidates will be assessed for linear regression model assumptions, normality of the residuals and homoscedasticity of residuals. Lastly, candidates will be compared for parsimony and assessed for multicollinearity. 

Data Cleaning Steps:

* exclude following two variables with values "0" for all observations.  
    * pnumlbw: previous number of low birth weight babies
    * pnumgsa: number of prior small for gestational age babies
* convert baby sex, father race, mother race, and malformations to factor variables

```{r}
# load birthweight dataset
bw = read_csv("./data/birthweight.csv")

# tidy dataset
bw = bw %>% 
  mutate(
    babysex = as.factor(recode(babysex, "1" = "male", "2" = "female")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8), labels = c("white", "black", "asian", "puerto_rican", "other")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4), labels = c("white", "black", "asian", "puerto_rican")),
    malform = factor(malform, levels = c(0, 1), labels = c("absent", "present"))) %>% 
  select(-pnumlbw, -pnumsga)
```

#### **Birthweight Histogram*

Distribution of the main response birthweight approximates a normal distribution.

```{r}
bw %>% 
  ggplot(aes(x = bwt)) +
  geom_histogram()
```

#### **Stepwise Backward Elimation**

Using an automatic search function that minimizes AIC using stepwise backward elimination, we obtain 11-predictor Candidate Model 1:

* AIC score: 48705.38
* bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken

```{r}
# Specify full and null models for forward & backward selection based on AIC Criteria
null = lm(bwt ~ 1, data = bw)
null

full = lm(bwt ~ ., data = bw)
full

# perform stepwise backward selection
step(full, data = bw, direction="backward")
```

#### **regsubsets()** - regression subset selection

The all possible regressions approach considers all possible subsets of the pool of explanatory variables and finds the model that best fits the data according to some criterion, such as R~2~(adj) and BIC. 

* Model containing explanatory variables *bhead, blength, delwt, frace:puerto_rican, gaweeks, mraceblack, ppbmi, and smoken* optimizes the **adjusted R~2~ criteria** and **BIC criteria**. 
* The top 5 models in the R~2~(adj) plot have roughly the same R~2~(adj). Since there is marginal improvement in R~2~(adj) by including frace:puerto_rican, we will exclude father race (frace) entirely from the candidate model. 
* Since we must include all factor levels within a variable, we will retain all factors for mother race (mrace). 

From this exploration, we obtain 7-predictor Candidate Model 2: 

bwt ~ bhead + blength + delwt + gaweeks + mrace + ppbmi + smoken

```{r}
leaps = leaps::regsubsets(bwt ~ ., data = bw)
# View the ranked models according to the adjusted r-squared criteria and BIC, respectively
# black indicates that a variable is included in the model, white not included. 
plot(leaps, scale = "adjr2")
plot(leaps, scale = "bic")

fit2 = lm(bwt ~ bhead + blength + delwt + gaweeks + mrace + ppbmi + smoken, data = bw)
summary(fit2)
```

#### Check Model Diagnostics for Candidate Models

1. Candidate Model 1:
bwt ~ bhead + blength + mrace + delwt + gaweeks + smoken + ppbmi + babysex + parity + ppwt + fincome

2. Candidate Model 2:
bwt ~ bhead + blength + delwt + gaweeks + mrace + ppbmi + smoken 

For both candidate models, we add residuals and predictions to the birthweight data using modelr package.

```{r}
# Candidate 1: Stepwise Backward Elimination Model, 11 predictors
fit_step = lm(bwt ~ bhead + blength + mrace + delwt + gaweeks + smoken + ppbmi + babysex + parity + ppwt + fincome, data = bw)
summary(fit_step)

# add predictions and residuals to stepwise model
bw_fit_step = modelr::add_predictions(bw, fit_step, var = "pred_step")
bw_fit_step = modelr::add_residuals(bw_fit_step, fit_step, var = "resid_step")

# Candidate 2: adjusted R-square Model, 7 predictors
fit_rs = lm(bwt ~ bhead + blength + delwt + gaweeks + mrace + ppbmi + smoken, data = bw)
summary(fit_rs)

# add predictions and residuals to adjusted r-square model
bw_fit_rs = modelr::add_predictions(bw, fit_rs, var = "pred_rs")
bw_fit_rs = modelr::add_residuals(bw_fit_rs, fit_rs, var = "resid_rs")
```

**1. Normality of Residuals (error distribution)**

* Assess for nearly normal residuals with mean 0
* Check using histogram of residuals and normal probability plot

Both candidate models have acceptable normality per Q-Q Plot and Histogram of Residuals

**1a) Candidate 1 QQ-Plot & Histogram**
```{r}
# evaluate normality of residuals with qq plot
qqnorm(fit_step$residuals)
qqline(fit_step$residuals)

# evaluate normality of residuals with histogram
hist(fit_step$residuals)
```

**1b) Candidate 2 QQ-Plot & Histogram**
```{r}
# evaluate normality of residuals with qq plot
qqnorm(fit_rs$residuals)
qqline(fit_rs$residuals)

# evaluate normality of residuals with histogram
hist(fit_rs$residuals)
```

**2. Homoscedasticity: Constant variance of residuals (errors)**

* Assess for constant variability of residuals 
* check using residuals plots of residuals vs. predicted value (e vs. y_hat)
    * residuals should be equally variable for low and high values of the predicted response variable
    * residuals randomly scattered in a band with a constant width around 0 (no fan shaped)
        
Both candidate models have acceptable homoscedasticity for birthweight values between 1500 to 4500 grams. Residuals for model predictions below ~1500grams are skewed above 0. The model may need to be finetuned for lower birthweight predictions.
```{r}
# Candidate 1: predicted value vs. residuals plot
bw_fit_step %>% 
  ggplot(aes(x = pred_step, y = resid_step)) +
  geom_point(alpha = 0.3) +
  labs(title = "Candidate Model 1")

# Candidate 2: predicted value vs. residuals plot
bw_fit_rs %>% 
  ggplot(aes(x = pred_rs, y = resid_rs)) +
  geom_point(alpha = 0.3) +
  labs(title = "Candidate Model 2")
```

#### Optimize Parsimony and Assess Multicollinearity

Both the Candidate Model 1 and Model 2 satisfied assumptions for 1) normality of residuals and 2) constant variance of residuals (between birthweights 1500-4500gram)

Optimizing for parsimony, we will select the 7-predictor Model 2 over the 11-predictor Model 1.

Lastly, we will check for multicollinearity in the 7 predictor model using a 1) correlation matrix and 2) VIF (variance inflation factor) Scores, which measure how much of the variance of a regression coefficient is inflated due to multicollinearity within the model. 

```{r}
# check for collinearity between numerical predictor variables
bw_fit_rs %>% 
  select(bwt, bhead, blength, delwt, gaweeks, ppbmi, smoken) %>% 
  cor() %>% 
  knitr::kable()

# Evaluate VIF Scores
car::vif(fit_rs) %>% knitr::kable()
```

We will drop variable pre-pregnancy BMI due to the high correlation coefficient (0.72) with variable delivery weight.

#### Final Proposed Model

Dropping pre-pregnancy bmi only marginally decreases the R~2~(adj), from 0.716 to 0.713. The final 6 Predictor Model satisfies regression assumptions for 1) normality of residuals and 2) constant variance of residuals

**Final Model**: bwt ~ bhead + blength + delwt + gaweeks + mrace + smoken

```{r}
# Final Model Coefficients
fit_final = lm(bwt ~ bhead + blength + delwt + gaweeks + mrace + smoken, data = bw)
summary(fit_final) %>% broom::tidy() %>% knitr::kable()
summary(fit_final) %>% broom::glance() %>% knitr::kable()

# Prediction vs Residuals Plot
bw_fit_final = modelr::add_predictions(bw, fit_final, var = "pred")
bw_fit_final = modelr::add_residuals(bw_fit_final, fit_final, var = "resid")

bw_fit_final %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3) +
  labs(title = "Final Proposed Model, Residuals Plot")

# histogram of residuals
bw_fit_final %>% 
  ggplot(aes(x = resid)) +
    geom_histogram()
```

### Problem 2.2 Cross-Validation

We will use cross validation to compare the final proposed model (fit_final) with a two predictor model (blength, gaweeks) and three-way interaction model (bhead, blength, babysex). 

Iteration for RMSE will be performed over 100 cross validation folds with a 80% training and 20% test split.

```{r}
# two predictor model
fit_2 = lm(bwt ~ blength, gaweeks, data = bw)

# 3 way interaction model
fit_3 = lm(bwt ~ bhead * blength * babysex, data = bw)
```

```{r}
cv_bw = modelr::crossv_mc(data = bw, n = 100, test = 0.2, id = "id")

cv_bw = cv_bw %>% 
  mutate(lm_final = map(train, ~lm(bwt ~ bhead + blength + delwt + gaweeks + mrace + smoken, data = .x)),
         lm_2pred = map(train, ~lm(bwt ~ blength, gaweeks, data = .x)),
         lm_3pred = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(rmse_final = map2_dbl(lm_final, test, ~rmse(model = .x, data = .y)),
         rmse_2pred = map2_dbl(lm_2pred, test, ~rmse(model = .x, data = .y)),
         rmse_3pred = map2_dbl(lm_3pred, test, ~rmse(model = .x, data = .y)))
```

The proposed model from our explanatory search has the lowest RMSE, in comparison to the 2 and 3 predictor model. We prefer the 6-predictor proposed model due to the strongest prediction accuracy. 
```{r}
cv_bw %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(
    model = str_replace(model, "rmse_", "model "),
    model = fct_reorder(model, rmse)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
    geom_violin() +
  labs(title = "Distribution of RMSE from Cross-Validation", 
    x = "Model", 
    y = "RMSE")
```

